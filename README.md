# Awesome-Optimizer
Collect optimizer related papers, data, repositories

| Title                                           |  Year    | Optimizer       | Published                                  | Code                                              | Keywords                                  |
| ---------------------- | ---------------------- | ---------|-------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------|
| Adam: A Method for Stochastic Optimization      | 2014     | Adam            | [arxiv](https://arxiv.org/abs/1412.6980) | [code](https://paperswithcode.com/paper/adam-a-method-for-stochastic-optimization) | Gradient descent    |
| Adam: A Method for Stochastic Optimization      | 2015     | AdaMax          | [arxiv](https://arxiv.org/abs/1412.6980) | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adamax.py#L5) | Gradient descent    |
| Adafactor: Adaptive Learning Rates with Sublinear Memory Cost | 2018     | Adafactor       | [arxiv](https://arxiv.org/abs/1804.04235) | [code](https://github.com/DeadAt0m/adafactor-pytorch) | Gradient descent    |
| Quasi-hyperbolic momentum and Adam for deep learning      | 2018     | QHAdam            | [arxiv](https://arxiv.org/abs/1810.06801) | [code](https://github.com/facebookresearch/qhoptim) | Gradient descent    |
|Online Adaptive Methods, Universality and Acceleration|2018|AcceleGrad|[arxiv](https://arxiv.org/abs/1809.02864)||Gradient descent|
| On the Convergence of Adam and Beyond           | 2019     | AMSGrad         | [arxiv](https://arxiv.org/abs/1904.09237) | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6) | Gradient descent    |
