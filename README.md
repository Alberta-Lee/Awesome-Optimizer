# Awesome-Optimizer
Collect optimizer related papers, data, repositories

| Title                                           |  Year    | Optimizer       | Published                                  | Code                                              | Keywords                                  |
| ---------------------- | ---------------------- | ---------|-------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------|
| Adam: A Method for Stochastic Optimization      | 2014     | Adam            | [arxiv](https://arxiv.org/abs/1412.6980) | [code](https://paperswithcode.com/paper/adam-a-method-for-stochastic-optimization) | Gradient descent    |
| Adam: A Method for Stochastic Optimization      | 2015     | AdaMax          | [arxiv](https://arxiv.org/abs/1412.6980) | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adamax.py#L5) | Gradient descent    |
|AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks|2017|AdaBatch|[arxiv](https://arxiv.org/abs/1712.02029)|[code](https://github.com/GXU-GMU-MICCAI/AdaBatch-numerical-experiments)|Gradient descent|
|AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training|2017|AdaComp|[arxiv](https://arxiv.org/abs/1712.02679)||Gradient descent|
| Adafactor: Adaptive Learning Rates with Sublinear Memory Cost | 2018     | Adafactor       | [arxiv](https://arxiv.org/abs/1804.04235) | [code](https://github.com/DeadAt0m/adafactor-pytorch) | Gradient descent    |
| Quasi-hyperbolic momentum and Adam for deep learning      | 2018     | QHAdam            | [arxiv](https://arxiv.org/abs/1810.06801) | [code](https://github.com/facebookresearch/qhoptim) | Gradient descent    |
|Online Adaptive Methods, Universality and Acceleration|2018|AcceleGrad|[arxiv](https://arxiv.org/abs/1809.02864)||Gradient descent|
|Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods|2018|AdaBayes|[arxiv](https://arxiv.org/abs/1807.07540)|[code](https://github.com/LaurenceA/adabayes)|Gradient descent|
| On the Convergence of Adam and Beyond           | 2019     | AMSGrad         | [arxiv](https://arxiv.org/abs/1904.09237) | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6) | Gradient descent    |
|Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates|2019|AdaAlter|[arxiv](https://arxiv.org/abs/1911.09030)|[code](https://github.com/xcgoner/AISTATS2020-AdaAlter-GluonNLP)|Gradient descent|
|Adaptive Gradient Methods with Dynamic Bound of Learning Rate|2019|AdaBound|[arxiv](https://arxiv.org/abs/1902.09843)|[code](https://github.com/Luolc/AdaBound)|Gradient descent|
|AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients|2020|AdaBelief|[arxiv](https://arxiv.org/abs/2010.07468)|[code](https://github.com/juntang-zhuang/Adabelief-Optimizer)|Gradient descent|
|Why are Adaptive Methods Good for Attention Models?|2020|ACClip|[arxiv](https://arxiv.org/abs/1912.03194)||Gradient descent|
